\documentclass[reqno]{amsart}
\input{cheatsheet.tex}

\begin{document}
    \title{Question 3}
    \author{Abhay Shankar K: cs21btech11001 \& Kartheek Tammana: cs21btech11028}
    \date{\today}
    \maketitle

    \begin{enumerate}[label=\textbf{(\Roman*)}]
        \item Derive the expression of likelihood and prior for a heteroscedastic setting for a single data point with input \(\mathbf{x_n}\) and output \(t_n\).
        
        Consider the following formula for the target variable:
        \[t = y(\mathbf{x, w}) + \epsilon(\mathbf{x})\] where \(\epsilon\) is a zero mean Gaussian random variable with precision \(\beta(\mathbf{x})\) and \(y\) is a deterministic function. 
        Due to heteroscedasticity, the Gaussian noise is dependent on the input \(\mathbf{x}\). 
        
        Due to the properties of Gaussian distribution, \(t\) is also normal, with its distrbution i.e. the likelihood function given by:

        \[p\brak{t_n | \mathbf{x_n}, \mathbf{w}, \boldsymbol{\beta}} = \sqrt{\frac{\beta_n}{2\pi}} \exp \cbrak{- \frac{\beta_n}{2} \brak{t_n - y( \mathbf{x_n, w})}^2}\]


        The prior is given by:
        \[p(\mathbf{w}) = \mathcal{N}(\mathbf{w} | \mathbf{m_0}, \mathbf{S_0})\]

        Here, \(m_0\) is the mean of the prior distribution and \(S_0\) is the covariance matrix.

        In a homoscedastic setting, we may assume that \(S_0 = \mathbf{\beta}^{-1} I\), where I is the identity matrix. However, in a heteroscedastic setting, we obviously cannot, and instead \(S_0\) will be an arbitrary diagonal matrix (because the variables are independent, but not identically distributed).

        \item Provide the expression for the objective function that you will consider for the ML and MAP estimation of the parameters considering a data set of size N.
        
        With \(\mathbf{X} = \seq{\mathbf{x}}{n}\), the objective function for ML estimation is given by:
        \begin{align}
            \cpr{\mathbf{t}}{\mathbf{X}, \mathbf{w}, \boldsymbol{\beta}} &= \prod_{n = 1}^{N} \sqrt{\frac{\beta_n}{2\pi}} \exp \cbrak{- \frac{\beta_n}{2} \brak{t_n - y(\mathbf{x_n, w})}^2} \\
            &= \mathcal{N} (\mathbf{t}|\mathbf{\Phi^Tw, R})\\
            \equiv \ln p &= \text{constant} + \frac{1}{2} \sum_{n = 1}^{N} \beta_n \brak{t_n - y(\mathbf{x_n, w})}^2 \label{eq:ml}
        \end{align}

        Defining the matrix \(\mathbf{R}\) with \(R_{ii} = \beta_i\).

        % which is equivalent to minimizing the weighted sum of squared errors.

        % The objective function for MAP estimation, \(\cpr{\mathbf{w}}{\mathbf{t}}\) is given by

        % \[\cpr{\mathbf{w}}{\mathbf{t}} = \mathcal{N} \brak{\mathbf{w | m_N, S_N}}\] where \begin{itemize}
        %     \item \(\mathbf{S_N} = (\mathbf{S_0}^{-1} + \beta \mathbf{\Phi^T \Phi})^{-1}\)
        %     \item \(\mathbf{m_N} = \mathbf{S_N} (S_0^{-1}m_0 + \beta \mathbf{\Phi^T t})\)
        % \end{itemize}

        % These formulae are as stated in the Bishop textbook, and the homoscedastic assumption is made after presenting the general equation. 
        
        %  \begin{align}
        %     \begin{split}
        %         \cpr{\mathbf{w}}{\mathbf{t}} &= \cpr{\mathbf{t}}{\mathbf{w}} p(\mathbf{w}) \\
        %             &= \brak{\prod_{n = 1}^{N} \mathcal{N} (t_n | y(\mathbf{x_n, w}), \beta_n^{-1})} \mathcal{N}(\mathbf{w} | \mathbf{m_0}, \mathbf{S_0})
        %     \end{split}
        % \end{align}

        % Substituting \(y(\mathbf{x_n, w}) = \mathbf{w^T}\boldsymbol{\phi}(\mathbf{x_n})\) and , we have

        % \begin{align}
        %     \begin{split}
        %         \cpr{\mathbf{w}}{\mathbf{t}} &= \mathcal{N} (\mathbf{t} | \mathbf{\Phi w}, \mathbf{R^{-1}}) \mathcal{N}(\mathbf{w} | \mathbf{m_0}, \mathbf{S_0}) \\
        %     \end{split}
        % \end{align}

        Given \begin{itemize}
            \item \[p(\mathbf{w}) = \mathcal{N}(\mathbf{m_0, S_0})\]
            \item \[p(t|w) = \mathcal{N}(\mathbf{\Phi^T w, R^{-1}})\]
        \end{itemize}

        The objective function for MAP estimation is given by the normal distrbution with mean \(\boldsymbol{\mu}\) and variance \(\boldsymbol{\Sigma}\) where 

        % \[p(\mathbf{w|t}) = \mathcal{N}(\mathbf{w}|\mu, \Sigma)\]
            \(\boldsymbol{\Sigma} = (\mathbf{S_0}^{-1} + \mathbf{\Phi^T R \Phi})^{-1}\)
            \(\boldsymbol{\mu} = \boldsymbol{\Sigma} (\mathbf{S_0}^{-1} \mathbf{m_0} + \mathbf{\Phi^T R t})\)

        Thus, we have \[p(\mathbf{w|t}) = \mathcal{N} \brak{\mathbf{w}|(\mathbf{S_0}^{-1} + \mathbf{\Phi^T R \Phi})^{-1}(\mathbf{S_0}^{-1} \mathbf{m_0} + \mathbf{\Phi^T R t}), (\mathbf{S_0}^{-1} + \mathbf{\Phi^T R \Phi})^{-1}}\]

        \item Show \[E_\mathcal{D} = \sum_{n = 1}^{N} r_n \cbrak{t_n - \mathbf{w^T \phi(x_n)}}^2 \] and find \(\mathbf{w}\) that minimizes \(E_\mathcal{D}\).
        
        Reframing ~\ref{eq:ml} in the style of the homoscedastic case, we have \[-\ln p(\mathcal{D} | \mathbf{w}) = \frac{N}{2} \ln \beta_n - \frac{N}{2} \ln (2\pi) - E_\mathcal{D}\] whence it is evident that \[E_\mathcal{D} = \inv{2} \sum_{n = 1}^{N} \beta_n \cbrak{t_n - \mathbf{w^T \phi(x_n)}}^2 \label{eq:ED} \]

        Setting \(r_n = \frac{\beta_n}{2}\), we obtain the desired equation.

        Now, we may obtain the \(\mathbf{w}\) that minimizes \(E_\mathcal{D}\) by differentiating ~\ref{eq:ED} and setting the derivative to zero like so:

        \[\sum_{n = 1}^{N} \beta_n \cbrak{t_n - \mathbf{w^T \phi(x_n)}} \mathbf{\phi\brak{x_n}^T} = 0\]

        Whence 

        \[ \sum_{n = 1}^{N} t_n \beta_n \phi_n^T = \mathbf{w^T}\sum_{n = 1}^{N} \beta_n \mathbf{\phi\brak{x_n}\phi\brak{x_n}^T}\]

        % and thus, 
        Let the matrix \(\mathbf{R}\) be a diagonal matrix with \(R_{ii} = \beta_i\). Then, we have

        \begin{align}
            \Phi^T R \mathbf{t} &= (\Phi^T R \Phi) w \\
            \implies \mathbf{w_{ML}} &= (\mathbf{\Phi^T R \Phi})^{-1} \mathbf{\Phi^T R t}
        \end{align}
        
    \end{enumerate}


\end{document}