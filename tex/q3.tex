\documentclass[reqno]{amsart}
\input{cheatsheet.tex}

\begin{document}
    \title{Question 3}
    \author{Abhay Shankar K: cs21btech11001 \& Kartheek Tammana: cs21btech11028}
    \date{\today}
    \maketitle

    \begin{enumerate}[label=\textbf{(\Roman*)}]
        \item \question Derive the expression of likelihood and prior for a heteroscedastic setting for a single data point with input \(\mathbf{x_n}\) and output \(t_n\).
        
        \solution
        \begin{enumerate}[label=\textbf{(\alph*)}]
            \item Consider the following formula for the target variable:
            \[t = \mathbf{w^T} \boldsymbol{\phi}(\mathbf{x}) + \epsilon(\mathbf{x})\] where \(\epsilon\) is a zero mean Gaussian random variable with precision \(\beta(\mathbf{x})\) and \(\boldsymbol{\phi}\) is a deterministic function. 
            Due to heteroscedasticity, the Gaussian noise is dependent on the input \(\mathbf{x}\). 
            
            Due to the properties of Gaussian distribution, \(t\) is also normal, with its distrbution i.e. the likelihood function given by:
            \begin{equation}
                p\brak{t_n | \mathbf{x}_n, \mathbf{w}, \boldsymbol{\beta}} 
                = 
                \sqrt{\frac{\beta_n}{2\pi}} \exp \cbrak{- \frac{\beta_n}{2} \brak{t_n - \mathbf{w^T} \boldsymbol{\phi}(\mathbf{x}_n)}^2}
                \label{eq:likelihood}
            \end{equation}
    
            So, 
            \[p\brak{t_n | \mathbf{x_n}, \mathbf{w}, \boldsymbol{\beta}} = \mathcal{N} (t_n | \mathbf{w^T} \boldsymbol{\phi}(\mathbf{x}_n), \beta^{-1})\]
            \item We may assume a Gaussian prior for \(\mathbf{w}\), with arbitrary mean \(\mathbf{m_0}\) and covariance matrix \(\mathbf{S_0}\) in which case the prior is given by
        \[p(\mathbf{w}) = \mathcal{N}(\mathbf{w} | \mathbf{m_0}, \mathbf{S_0})\]

        \end{enumerate}

        \item \question Provide the expression for the objective function that you will consider for the ML and MAP estimation of the parameters considering a data set of size N.

        \solution
        \begin{enumerate}[label=\textbf{(\alph*)}]
            \item To express this more succinctly, we define the design matrix \[\boldsymbol{\Phi} = \myvec{\boldsymbol{\phi}(\mathbf{x}_1)^T \\ \vdots \\ \boldsymbol{\phi}(\mathbf{x}_n)^T} \]
            the data set \(\mathbf{X} = \seq{\mathbf{x}}{n}\)
            and the diagonal weighing matrix \(\mathbf{R}\) with \(R_{ii} = \beta_i\).
            
            Thus, the objective function for ML estimation is given by:
            \begin{align}
                \begin{split}
                    \cpr{\mathbf{t}}{\mathbf{X}, \mathbf{w}, \boldsymbol{\beta}}  
                    &= \prod_{n = 1}^{N} p\brak{t_n | \mathbf{x_n}, \mathbf{w}, \boldsymbol{\beta}} \\
                    &= \prod_{n = 1}^{N} \mathcal{N} (t_n | \mathbf{w^T} \boldsymbol{\phi}(\mathbf{x}_n), \beta_n^{-1}) \\
                    &= \exp \cbrak{- \frac{1}{2} \sum_{n = 1}^{N} \beta_n \brak{t_n - \mathbf{w^T} \boldsymbol{\phi}(\mathbf{x}_n)}^2} \\
                    &= \exp \cbrak{- \frac{1}{2} \brak{\mathbf{t} - \boldsymbol{\Phi^T w}}^T \mathbf{R} \brak{\mathbf{t} - \boldsymbol{\Phi^T w}}} \\
                    &= \mathcal{N} (\mathbf{t}|\mathbf{\Phi w, R^{-1}}) \label{eq:ml} 
                \end{split}
                % \equiv \ln p &= \text{constant} + \frac{1}{2} \sum_{n = 1}^{N} \beta_n \brak{t_n - y(\mathbf{x_n, w})}^2 
            \end{align}
            \item 
                Given
                    \[p(\mathbf{w}) = \mathcal{N}(w|\mathbf{m_0, S_0})\]
                and
                    \[p(\mathbf{t|w}) = \mathcal{N}(\mathbf{\Phi w, R^{-1}})\]

                Let \(\mathbf{z} = \myvec{\mathbf{w} \\ \mathbf{t}}\) as follows:
                \begin{align}
                    \begin{split}
                        \ln p(\mathbf{z}) &= \ln p(\mathbf{w}) + \ln p(\mathbf{t|w}) \\
                            &= -\inv{2} (\mathbf{w - m_0})^T \mathbf{S_0}^{-1} (\mathbf{w - m}_0) \\
                            &\ - \inv{2} (\mathbf{t - \Phi w})^T \mathbf{R} (\mathbf{t - \Phi w}) + \text{constant} \label{eq:map1}
                    \end{split}
                \end{align}

                Furthermore, due to linearity of expectation, we have 
                \[\ev{\mathbf{z}} = \myvec{\ev{\mathbf{w}} \\ \ev{\mathbf{t}}}\]
                from which
                \[\cov{\mathbf{z}} = \myvec{
                    \var{\mathbf{w}} & \cov{\mathbf{w}, \mathbf{t}} \\
                    \cov{\mathbf{t}, \mathbf{w}} & \var{\mathbf{t}}
                }\]

                From ~\eqref{eq:map1}, it is clear that \(p(\mathbf{z})\) is a Gaussian distribution. Now we complete the square.
                
                To find the covariance of \(\mathbf{w|t}\), we consider the single term of second order in \(\mathbf{w}\) from ~\eqref{eq:map1}: 

                \[\inv{2} \mathbf{w^T \Sigma}^{-1}\mathbf{w} = \inv{2} \mathbf{w^T}(\mathbf{S_0}^{-1} + \mathbf{\Phi R \Phi^T})\mathbf{w} \]

                We treat \(\mathbf{t}\) as a constant.

                Thus, the covariance is given by \[\mathbf{\Sigma} = (\mathbf{S_0}^{-1} + \mathbf{\Phi^T R \Phi})^{-1}\]

                Similarly, we may obtain \(\boldsymbol{\mu}\) using the terms of ~\eqref{eq:map1} of first order in \(\mathbf{w}\). We have

                \[\mathbf{w^T \Sigma}^{-1} \boldsymbol{\mu} = \mathbf{w^T} \mathbf{S_0}^{-1} \mathbf{m_0} + \mathbf{w^T \Phi^T R t}\]
                    which yields
                \[\boldsymbol{\mu} = \boldsymbol{\Sigma} (\mathbf{S_0}^{-1} \mathbf{m_0} + \mathbf{\Phi^T R t})\]

                Thus, the MAP objective function is \[p(\mathbf{w|t}) = \mathcal{N} (\mathbf{w|\boldsymbol{\mu}, \Sigma})\]
                
        \end{enumerate}

        

        \item \question Show \[E_\mathcal{D} = \sum_{n = 1}^{N} r_n \cbrak{t_n - \mathbf{w^T \phi(x_n)}}^2 \] and find \(\mathbf{w}\) that minimizes \(E_\mathcal{D}\).
        \solution
        \begin{enumerate}[label=\textbf{(\alph*)}]
            \item Taking logarithm of ~\eqref{eq:likelihood}
            \[-\ln p(\mathcal{D} | \mathbf{w}) = \frac{N}{2} \ln \beta_n - \frac{N}{2} \ln (2\pi) + E_\mathcal{D}\]
            with
            \begin{equation}
                E_\mathcal{D} = \inv{2} \sum_{n = 1}^{N} \beta_n \cbrak{t_n - \mathbf{w^T \phi(x_n)}}^2 \label{eq:ED}
            \end{equation}
    
            Setting \(r_n = \frac{\beta_n}{2}\), we obtain the desired equation.

            \item Now, we may obtain the \(\mathbf{w}\) that minimizes \(E_\mathcal{D}\) by differentiating ~\eqref{eq:ED} and setting the derivative to zero like so:
        \[\sum_{n = 1}^{N} \beta_n \cbrak{t_n - \mathbf{w^T \phi(x_n)}} \mathbf{\phi\brak{x_n}^T} = 0\]
        Whence 
        \[ \sum_{n = 1}^{N} t_n \beta_n \mathbf{\phi\brak{x_n}}^T = \mathbf{w^T}\sum_{n = 1}^{N} \beta_n \mathbf{\phi\brak{x_n}\phi\brak{x_n}^T}\]

        Using the matrix \(\mathbf{R}\) defined as before, taking the transpose of both sides yields
        \begin{align}
            \begin{split}
                \mathbf{\Phi^T R t} &= \mathbf{(\Phi^T R \Phi) w} \\
                \implies \mathbf{w_{ML}} &= (\mathbf{\Phi^T R \Phi})^{-1} \mathbf{\Phi^T R t}
            \end{split}
        \end{align}
        \end{enumerate}
        
    \end{enumerate}


\end{document}