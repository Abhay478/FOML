\documentclass{amsart}
\input{~/Code/Lang/Latex/cheatsheet.tex}

\begin{document}
    \title{Question 3}
    \author{Abhay Shankar K: cs21btech11001 \& Kartheek Tammana: cs21btech11028}
    \date{\today}
    \maketitle

    \begin{enumerate}[label=\textbf{(\Roman*)}]
        \item Derive the expression of likelihood and prior for a heteroscedastic setting for a single data point with input \(x_n\) and output \(t_n\).
        
        Consider the following formula for the target variable:
        \[t = y(\mathbf{x, w}) + \epsilon(\mathbf{x})\] where \(\epsilon\) is a zero mean Gaussian random variable with precision \(\beta(\mathbf{x})\). 
        
        Due to heteroscedasticity, the Gaussian noise is dependent on the input \(\mathbf{x}\). Due to the properties of Gaussian distribution, \(t\) is also normal, with its distrbution i.e. the likelihood function given by:

        \[p\brak{t_n | x_n, \mathbf{w}} = \sqrt{\frac{\beta_n}{2\pi}} \exp \cbrak{- \frac{\beta_n}{2} \brak{t_n - y(x_n, \mathbf{w})}^2}\]


        The prior is given by:
        \[p(\mathbf{w}) = \mathcal{N}(\mathbf{w} | \mathbf{m_0}, \mathbf{S_0})\]

        Here, \(m_0\) is the mean of the prior distribution and \(S_0\) is the covariance matrix.

        In a homoscedastic setting, we may assume that \(S_0 = \mathbf{\beta}^{-1} I\), where I is the identity matrix. However, in a heteroscedastic setting, we obviously cannot, and instead \(S_0\) will be an arbitrary diagonal matrix (because the variables are independent, but not identically distributed).

        \item Provide the expression for the objective function that you will consider for the ML and MAP estimation of the parameters considering a data set of size N.
        
        The objective function for ML estimation is given by:
        \begin{align}
            \cpr{\mathcal{D}}{\mathbf{w}, \boldsymbol{\beta}} &= \prod_{n = 1}^{N} \sqrt{\frac{\beta_n}{2\pi}} \exp \cbrak{- \frac{\beta_n}{2} \brak{t_n - y(x_n, \mathbf{w})}^2} \\
            \equiv \ln p &= \text{constant} + \frac{1}{2} \sum_{n = 1}^{N} \beta_n \brak{t_n - y(x_n, \mathbf{w})}^2 \label{eq:ml}
        \end{align}

        which is equivalent to minimizing the weighted sum of squared errors.

        The objective function for MAP estimation, \(\cpr{\mathbf{w}}{\mathcal{D}} =  \cpr{\mathcal{D}}{\mathbf{w}} \spr{\mathbf{w}}\) is given by:

        \begin{align}
            \cpr{\mathcal{D}}{\mathbf{w}} \spr{\mathbf{w}} &= \prod_{n = 1}^{N} \sqrt{\frac{\beta_n}{2\pi}} \exp \cbrak{- \frac{\beta_n}{2} \brak{t_n - y(x_n, \mathbf{w})}^2} \mathcal{N}(\mathbf{w} | \mathbf{m_0}, \mathbf{S_0}) \\
            \equiv \ln p(\mathcal{D} | \mathbf{w}) + \ln p(\mathbf{w}) &=\\  \text{constant} &- \frac{1}{2} \sum_{n = 1}^{N} \beta_n \brak{t_n - y(x_n, \mathbf{w})}^2 + \frac{1}{2} (\mathbf{w - m_0})^T \mathbf{S_0}^{-1} (\mathbf{w - m_0})
        \end{align}

        \item Show \[E_\mathcal{D} = \sum_{n = 1}^{N} r_n \cbrak{t_n - \mathbf{w^T \phi(x_n)}}^2 \] and find \(\mathbf{w}\) that minimizes \(E_\mathcal{D}\).
        
        Reframing ~\ref{eq:ml} in the style of the homoscedastic case, we have \[-\ln p(\mathcal{D} | \mathbf{w}) = \frac{N}{2} \ln \beta_n - \frac{N}{2} \ln (2\pi) - E_\mathcal{D}\] whence it is evident that \[E_\mathcal{D} = \inv{2} \sum_{n = 1}^{N} \beta_n \cbrak{t_n - \mathbf{w^T \phi(x_n)}}^2 \label{eq:ED} \]

        Setting \(r_n = \frac{\beta_n}{2}\), we obtain the desired equation.

        Now, we may obtain the \(\mathbf{w}\) that minimizes \(E_\mathcal{D}\) by differentiating ~\ref{eq:ED} and setting the derivative to zero like so:

        \[\sum_{n = 1}^{N} \beta_n \cbrak{t_n - \mathbf{w^T \phi(x_n)}} \mathbf{\phi\brak{x_n}^T} = 0\]

        Whence 

        \[ \sum_{n = 1}^{N} t_n \beta_n \phi_n^T = \mathbf{w^T}\sum_{n = 1}^{N} \beta_n \mathbf{\phi\brak{x_n}\phi\brak{x_n}^T}\]

        % and thus, 
        Let the matrix \(\mathbf{R}\) be a diagonal matrix with \(R_{ii} = \beta_i\). Then, we have

        \begin{align}
            \Phi^T R \mathbf{t} &= (\Phi^T R \Phi) w \\
            \implies \mathbf{w_{ML}} &= (\mathbf{\Phi^T R \Phi})^{-1} \mathbf{\Phi^T R t}
        \end{align}
        
    \end{enumerate}


\end{document}