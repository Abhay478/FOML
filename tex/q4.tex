\documentclass[reqno]{amsart}
\input{cheatsheet.tex}

\begin{document}
    \title{Question 4}
    \author{Abhay Shankar K: cs21btech11001 \& Kartheek Tammana: cs21btech11028}
    \maketitle

    \begin{enumerate}[label=\textbf{(\Roman*)}]
        \item \question Provide the expressions of the gradient, Hessian, and update equations for the Newton-Raphson optimization technique used to obtain the parameters in the logistic regression model. Provide an algorithm describing the methodology.
        
        \solution
        Knowing the maximum likelihood function, 
        \[\cpr{\mathbf{t}}{\mathbf{w}} = \prod_{n = 1}^{N} y_n^{t_n} (1 - y_n)^{t_n}\]
        we can obtain the cross-entropy error function by taking negative logarithm: 
        \begin{equation}
            E(\mathbf{w}) = - \sum_{n = 1}^{N} \brak{t_n \ln y_n + (1 - t_n) \ln (1 - y_n)} \label{eq:cross-entropy}
        \end{equation}
        
        Note that \(y_n = \sigma(\mathbf{w^T}\boldsymbol{\phi}_n)\) where \(\sigma\) is the sigmoid function and \(\boldsymbol{\phi}_n\) are the features. 

        Furthremore, we have the design matrix \(\mathbf{\Phi}\) with \(\boldsymbol{\phi}_n^T\) as its n'th row.

        Taking gradients of ~\eqref{eq:cross-entropy} with respect to \(\mathbf{w}\), we have
        \begin{itemize}
            \item Gradient of the error: \begin{align}
                \begin{split}
                    \nabla E(\mathbf{w}) &= - \sum_{n = 1}^{N} t_n \cdot \inv{y_n} y_n (1 - y_n) \phi_n - (1 - t_n) \cdot \inv{1 - y_n} y_n (1 - y_n) \phi_n \\
                    &= \sum_{n = 1}^{N} \brak{y_n - t_n} \phi_n \\
                    &= \mathbf{\Phi^T(y - t)}
                \end{split}
            \end{align}
            \item The Hessian: \[\nabla \nabla E(\mathbf{w}) = \sum_{n = 1}^{N} y_n (1 - y_n) \phi_n \phi_n^T = \mathbf{\Phi^T R \Phi}\] where R is the diagonal matrix given by \(R_{nn} = y_n (1 - y_n)\)
            \item Update function: \[\mathbf{w}^{(new)} = \mathbf{w}^{(old)} - \mathbf{H}^{-1} \nabla E(\mathbf{w}) = \]
            \begin{align}
                \begin{split}
                    \mathbf{w}^{(new)} &= \mathbf{w}^{(old)} - \mathbf{H}^{-1} \nabla E(\mathbf{w}) \\
                            &= \mathbf{w}^{(old)} - (\mathbf{\Phi^T R \Phi})^{-1} \mathbf{\Phi^T(y - t)} \\
                            &= (\mathbf{\Phi^T R \Phi})^{-1} \brak{\mathbf{\Phi^T R \Phi w}^{(old)} - \mathbf{\Phi^T(y - t)}} \\
                            &= (\mathbf{\Phi^T R \Phi})^{-1} \mathbf{\Phi^T R z} \label{eq:update}
                \end{split}
            \end{align}

            with \[\mathbf{z} = \mathbf{\Phi w}^{(old)} - \mathbf{R^{-1} (y - t)}\]
        \end{itemize}

        The algorithm for update, implemented in python:

        \begin{lstlisting}[language=Python]
import numpy as np
def update(w, Phi, t):
    y =  mp.array([sigmoid(p @ w) for p in Phi])
    R = np.diag(y * (1 - y))
    z = Phi @ w - np.linalg.inv(R) @ (y - t)
    return np.linalg.inv(Phi.T @ R @ Phi) @ Phi.T @ R @ z
        \end{lstlisting}

        \item 
        
        Modifying ~\eqref{eq:update}, we get 
        \[(\mathbf{\Phi^T R \Phi}) \mathbf{w} = \mathbf{\Phi^T R z}\]

        which is the normal equation for weighted least squares. Thus, the new weight vector \(\mathbf{w}^{(new)}\) is the solution to the weighted least squares problem with \(\mathbf{z}\) and the weighing matrix \(\mathbf{R}\). However, the weighing matrix \(R\) is not constant, but depends on the parameter vector \(\mathbf{w}\).
        
        Thus, we must apply the normal equations iteratively, each time using the new weight vector \(\mathbf{w}\) to compute a revised weighing matrix \(R\). So, the algorithm is known as iterative reweighted least squares (IRLS).
        
        \item 
        With the Hessian: \[\mathbf{H} = \mathbf{\Phi^T R \Phi}\]

        We know that a function is convex if its Hessian is positive definite. Thus, it is sufficient to prove positive-definiteness of the Hessian.

        Expanding \( \mathbf{u^T H u} \), we can prove positive-definiteness \begin{align}
            \begin{split}
                \mathbf{u^T H u} &= \mathbf{u^T \Phi^T R \Phi u} \\
                &= (\mathbf{u^T \Phi^T}) \mathbf{R} (\mathbf{\Phi u}) \\
                &= \sum_{n = 1}^{N} u_n \mathbf{\phi_n}^T R_{nn} \mathbf{\phi_n} u \\
                &= \sum_{n = 1}^{N} R_{nn} \norm{\phi_n}^2 \brak{u_n}^2 \\
                &> 0
            \end{split}
        \end{align}

        We have used the fact that \(R_{nn} > 0\) and \(\mathbf{u} \neq 0\). Thus, the Hessian is positive-definite, and the function is convex with unique minimum.


    \end{enumerate}
\end{document}