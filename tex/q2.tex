\documentclass[reqno]{amsart}
\input{cheatsheet.tex}

\begin{document}
    \title{Question 2}
    \author{Abhay Shankar K: cs21btech11001}
    \author{Kartheek Tammana: cs21btech11028}
    \date{\today}
    
    \maketitle

    \begin{enumerate}[label=\textbf{(\Roman*)}]
        \item \begin{enumerate}[label=\textbf{(\alph*)}]
            \item \begin{itemize}
                \item The paper proposes two models for ordinal data, namely the proportional odds model and the proportional hazards model.
                \item The proportional odds model is a generalisation of the logistic regression model for ordinal data. Here, the cumulative odds of the response variable \(Y \leq j\) are given by 
                \[\boldsymbol{\kappa}_j = \kappa_j \exp (- \boldsymbol{\beta^T} \mathbf{x})\]
                with 
                \[\boldsymbol{\kappa}_j = \frac{\gamma_j}{1 - \gamma_j}\]
                and
                \[\gamma_j = \sum_{i = 1}^{j} \pi_j\]
                where \(\pi_j\) is the probability of the \(j\)'th category of the response variable \(Y\), and \(\mathbf{x}\) is the covariant vector.
                
                The paper also defines the cumulative odds ratio \(\boldsymbol{\kappa}_{j, j + 1} = \frac{\boldsymbol{\kappa}_j}{\boldsymbol{\kappa}_{j + 1}}\), which is the odds of the \(j\)'th category over the \(j + 1\)'th category.

                \item The proportional hazards model considers a hazard function \(\lambda(t) = \), which expresses the probability of failure at time \(t\), of the form \[\lambda(t) = \lambda_0(t) \exp \brak{- \boldsymbol{\beta^T} \mathbf{x}}\]
                
                From this, we define the Survival function \(S(t) = \exp \brak{\Lambda_0 \exp \brak{- \boldsymbol{\beta^T} \mathbf{x}} dt}\) with \(\Lambda_0(t) = - \int_{0}^{t} \lambda_0(t) dt \) which represents the probabiltiy of surviving beyond time t.

                For discrete data, we define \(\gamma_j\) to be the cumulative hazard function, and can write the logarithm of the Survival function as \[\ln \sbrak{- \ln \brak{1 - \gamma_j(\mathbf{x})}} = \theta_j - \boldsymbol{\beta}^T \mathbf{x}\] which is known as the complementary log-log transform.


                \item The paper proposes a generalised empirical logit transform, as a generalisation of the two models. The quantity \(Z_{i} = \Sigma_j w_j \tilde{\lambda}_{ij}\), with 
                weights
                \[w_j \propto R_{.j}(n - R_{.j})(n_{.j} + n_{.j+1})\]
                and the logit transform
                \[\tilde{\lambda}_{ij} = \ln \brak{\frac{R_{ij} + \inv{2}}{n_i - R_{ij} + \inv{2}}}\]
                where the \(R\) terms are various cumulatives of empirical data, is called the generalised empirical logit transform for the \(i\)'th group.
                
                \item The paper also discusses \begin{itemize}
                    \item The properties of the two models, proposing a few alternative link functions.
                    \item Invariances of the models under reversal of the ordering.
                    \item Asymptotic properties of the two models.
                    \item Parameter estimation for both models.
                    \item Application of the models to real data.
                \end{itemize} 
            \end{itemize}
            \item Differences between Ordinal Regression, Multiclass classification and Linear Regression.
    
           \begin{center}
             \begin{tabular}{|c|c|c|c|}
                \hline
                &Ordinal & Multiclass & Linear \\
                \hline
                Response variable & Ordinal & Categorical & Continuous \\
                Link function & Logit & Softmax & Identity \\
                Loss function & Cross-entropy & Cross-entropy & Mean squared error \\
                Optimisation technique & GD & GD & Closed form \\
                Categories & Categories have relative order & Classes distinct & No classes \\ 
                \hline
             \end{tabular}
           \end{center}

           They all have different likelihood functions also:

           \begin{itemize}
            \item Ordinal regression: Given in next part.
            \item Logistic Multiclass classification: \[p(\mathbf{t}|\pi, \boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \mathbf{\Sigma}) = \prod_{n = 1}^{N} \sbrak{\pi \mathcal{N} \brak{\mathbf{x}_n|\boldsymbol{\mu}_1, \mathbf{\Sigma}}}^{t_n}\sbrak{(1 - \pi)\mathcal{N} \brak{\mathbf{x}_n|\boldsymbol{\mu}_2, \mathbf{\Sigma}}}^{1 - t_n} \] 
            \item Linear Regression: \[p(\mathbf{t}|\mathbf{X, w}, \boldsymbol{\beta}) = \mathcal{N}\brak{\mathbf{t}|\mathbf{w^T}\boldsymbol{\phi}(\mathbf{x}_n), \boldsymbol{\beta}^{-1}}\]
           \end{itemize}

           Where all the variables have the usual meaning.

        %    Also, only ordinal regression expresses the output distribution with cumulative odds of the response variable, oth the others use .

        In the ordinal regression models, the cumulative odds of the output variable are expressed as exponential in the weights \(\boldsymbol{\beta}\) and the covariant vector \(\mathbf{x}\). 
        
        In the logistic regression model, the probability distribution of the output is expressed as exponential in \(\boldsymbol{\beta}^T \mathbf{x}\). 
        
        In the linear regression model, the probability distribution of the output is linear in the weights. 
           
        \end{enumerate}
        \item \question Parameter Estimation
        
        \solution
    Revising the notation from the paper, we have the probabilities of the \(k\) ordered categories of the response variable \(Y\) given by \(\seq{\pi}{k}\), as a function of the covariant vector \(\mathbf{x}\), and their cumulative probabilities given by \(\displaystyle \gamma_j = \sum_{i = 1}^{j} \pi_j\). The cumulative odds are thus \(\boldsymbol{\kappa}_j = \frac{\gamma_j}{1 - \gamma_j}\).

    We then have the likelihood function \(\boldsymbol{\kappa}_j = \kappa_j \exp \brak{\boldsymbol{\beta^T}\mathbf{x}}\), which we can reframe as \begin{align}
        \begin{split}
            \frac{\gamma_j}{1 - \gamma_j} &= \exp \brak{\theta_j - \boldsymbol{\beta^T}\mathbf{x}} \\
            \implies \gamma_{j} &= \inv{1 + \exp \brak{\boldsymbol{\beta^T}\mathbf{x} - \theta_j}} 
        \end{split}
    \end{align}
    where we set \(\theta_0 = 0\).
    We also define \(\displaystyle R_j = \sum_{i = 1}^{j} n_i\).

    We have the likelihood function:
    \begin{align}
        \begin{split}
            p(\mathbf{y}|\mathbf{x}, \boldsymbol{\beta}) &= \prod_{j = 1}^{k} \pi_j^{n_j} \\
            &= \pi_1^{n_1} \prod_{j = 1}^{k-1} (\gamma_{j + 1} - \gamma_j)^{n_{j + 1}} \\
            &= \pi_1^{n_1} \prod_{j = 1}^{k-1} (\gamma_{j + 1} - \gamma_j)^{R_{j + 1} - R_j}  \\
            &= \prod_{n = 1}^{k-1} \brak{\frac{\gamma_j}{\gamma_{j + 1}}}^{R_j} \brak{1 - \frac{\gamma_j}{\gamma_{j + 1}}}^{R_{j + 1} - R_j} \\
        \end{split}
    \end{align}

    Taking the logarithm, we have
    \begin{align}
        \begin{split}
            -\ln p(\mathbf{y}|\mathbf{x}, \boldsymbol{\beta}) &= 
            \sum_{j = 1}^{k} \sbrak{R_j \ln \brak{\frac{1 + \exp \brak{\boldsymbol{\beta^T}\mathbf{x} - \theta_j}}{1 + \exp \brak{\boldsymbol{\beta^T}\mathbf{x} - \theta_{j+1}}}} 
            - (R_{j + 1} - R_j) \brak{\ln \brak{\frac{\e{-\theta_{j + 1}} - \e{-\theta_j}}{1 + \exp \brak{\boldsymbol{\beta^T}\mathbf{x} - \theta_{j+1}}}} + \boldsymbol{\beta^T}\mathbf{x}}} \\
        \end{split}
    \end{align}

    We can use gradient descent to find the optimal weights \(\boldsymbol{\beta}\) or intervals \(\boldsymbol{\theta} = \myvec{\theta_1 & \ldots & \theta_{k - 1}}^T\). However, the paper does not provide the gradient of the likelihood function due to its complexity.

    \item Code
    
    Refer \texttt{code/q2.ipynb}
    \end{enumerate}



\end{document}